{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11588546,"sourceType":"datasetVersion","datasetId":7266417},{"sourceId":13527333,"sourceType":"datasetVersion","datasetId":8589231}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:50:59.488927Z","iopub.execute_input":"2025-10-28T05:50:59.489177Z","iopub.status.idle":"2025-10-28T05:51:01.380269Z","shell.execute_reply.started":"2025-10-28T05:50:59.489138Z","shell.execute_reply":"2025-10-28T05:51:01.379352Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/SHA256SUMS.txt\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/CHANGELOG.txt\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/LICENSE.txt\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/outputevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/procedureevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/datetimeevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/ingredientevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/chartevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/inputevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/icustays.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/icu/d_items.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/poe.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/d_labitems.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/drgcodes.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/d_icd_diagnoses.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/emar.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/omr.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/emar_detail.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/pharmacy.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/d_hcpcs.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/prescriptions.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/d_icd_procedures.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/hcpcsevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/patients.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/transfers.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/diagnoses_icd.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/services.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/microbiologyevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/poe_detail.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/admissions.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/labevents.csv\n/kaggle/input/mimic-iv-2-1/mimic-iv-2.1/hosp/procedures_icd.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q torch==2.1.2+cu121 torchvision==0.16.2+cu121 torchaudio==2.1.2+cu121 \\\n  scikit-learn==1.6.0 pandas numpy tqdm umap-learn --upgrade \\\n  -f https://download.pytorch.org/whl/torch_stable.html\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:57:23.054337Z","iopub.execute_input":"2025-10-28T05:57:23.054689Z","iopub.status.idle":"2025-10-28T05:57:47.109645Z","shell.execute_reply.started":"2025-10-28T05:57:23.054661Z","shell.execute_reply":"2025-10-28T05:57:47.108833Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.0 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntransformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nprint(\"✅ CUDA available:\", torch.cuda.is_available())\nprint(\"GPU:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:58:20.606898Z","iopub.execute_input":"2025-10-28T05:58:20.607654Z","iopub.status.idle":"2025-10-28T05:58:22.651221Z","shell.execute_reply.started":"2025-10-28T05:58:20.607626Z","shell.execute_reply":"2025-10-28T05:58:22.650344Z"}},"outputs":[{"name":"stdout","text":"✅ CUDA available: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# 1) Imports & Config\n# ============================================================\nimport os, math, random, json\nimport numpy as np, pandas as pd\nfrom tqdm import tqdm\nimport torch, torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import (\n    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n    normalized_mutual_info_score as NMI, adjusted_rand_score as ARI\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nclass CFG:\n    SEED = 42\n    DATA_PATH = \"/kaggle/input/mimic-iv-2-1/mimic-iv-2.1\"   # update to your path\n    SAVE_DIR = \"/kaggle/working\"\n    MAX_LEN = 512\n    BATCH_SIZE = 64\n    LR = 2e-4\n    EPOCHS = 50\n    D_MODEL = 256\n    N_LAYERS = 4\n    DROPOUT = 0.1\n    MASK_PROB = 0.15\n    N_CLUSTERS = 8\n\ndef set_seed(seed):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed(CFG.SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:19:22.283082Z","iopub.execute_input":"2025-10-28T06:19:22.283794Z","iopub.status.idle":"2025-10-28T06:19:22.291741Z","shell.execute_reply.started":"2025-10-28T06:19:22.283769Z","shell.execute_reply":"2025-10-28T06:19:22.290712Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ============================================================\n# 2) Build preprocessed Orphanet subset directly from raw MIMIC-IV\n# ============================================================\nimport pandas as pd, numpy as np, gc\nfrom collections import Counter\n\nBASE = \"/kaggle/input/mimic-iv-2-1/mimic-iv-2.1\"\nprint(\"📂 Reading MIMIC-IV tables from:\", BASE)\n\n# --- Core hospital tables ---\nadm = pd.read_csv(f\"{BASE}/hosp/admissions.csv\",\n                  usecols=[\"subject_id\",\"hadm_id\",\"admittime\",\"dischtime\",\"hospital_expire_flag\"],\n                  parse_dates=[\"admittime\",\"dischtime\"])\ndx = pd.read_csv(f\"{BASE}/hosp/diagnoses_icd.csv\",\n                 usecols=[\"subject_id\",\"hadm_id\",\"icd_code\",\"icd_version\"])\nicustays = pd.read_csv(f\"{BASE}/icu/icustays.csv\",\n                       usecols=[\"subject_id\",\"hadm_id\"]).drop_duplicates()\n\n# --- Orphanet codes ---\norph = pd.read_csv(\"/kaggle/input/orpha-codes/icd10_diseases.csv\")\norph[\"ICDcodes\"] = orph[\"ICDcodes\"].astype(str).str.strip().str.upper()\nrare_icd = set(orph[\"ICDcodes\"].unique())\n\n# --- Filter diagnoses to Orphanet codes ---\ndx[\"icd_code\"] = dx[\"icd_code\"].astype(str).str.strip().str.upper()\nrare_dx = dx[dx[\"icd_code\"].isin(rare_icd)]\nrare_hadm_ids = rare_dx[\"hadm_id\"].unique()\nrare_subjects = rare_dx[\"subject_id\"].unique()\nprint(f\"✅ Orphanet rare-disease subset: {len(rare_subjects)} patients | {len(rare_hadm_ids)} admissions\")\n\n# --- Filter admissions to those IDs ---\nadm = adm[adm[\"hadm_id\"].isin(rare_hadm_ids)].copy()\nadm = adm.dropna(subset=[\"admittime\",\"dischtime\"])\nadm[\"los_days\"] = (adm[\"dischtime\"] - adm[\"admittime\"]).dt.total_seconds()/86400.0\nadm[\"los_days\"] = adm[\"los_days\"].clip(lower=0.01, upper=60)\nadm[\"los_days_log\"] = np.log1p(adm[\"los_days\"])\nadm[\"in_icu\"] = adm[\"hadm_id\"].isin(icustays[\"hadm_id\"]).astype(int)\nprint(\"Admissions shape:\", adm.shape)\n\n# --- Per-patient sequence assembly (simplified) ---\nadm = adm.sort_values([\"subject_id\",\"admittime\"]).reset_index(drop=True)\nsubjects = adm[\"subject_id\"].unique()\nN = len(subjects)\n\n# Use LOS, ICU flag, and temporal order to build simple sequence feature matrix\n# For each admission, create [LOS_log, ICU_flag, Expired_flag]\nfeatures = [\"los_days_log\",\"in_icu\",\"hospital_expire_flag\"]\nD = len(features)\nT_max = adm.groupby(\"subject_id\").size().max()\nT_cap = min(60, T_max)   # cap to reasonable sequence length\n\nX = np.zeros((N, T_cap, D), dtype=np.float32)\nM = np.zeros((N, T_cap), dtype=np.float32)\nY_ICU = np.zeros(N, dtype=np.float32)\nY_LOS = np.zeros(N, dtype=np.float32)\n\nfor i, sid in enumerate(subjects):\n    g = adm[adm[\"subject_id\"] == sid].head(T_cap)\n    Xi = g[features].to_numpy(dtype=np.float32)\n    L = Xi.shape[0]\n    X[i, :L, :] = Xi\n    M[i, :L] = 1.0\n    Y_ICU[i] = g[\"in_icu\"].max()\n    Y_LOS[i] = np.log1p(g[\"los_days\"].sum())\n\nprint(f\"✅ Final tensors built: X {X.shape}, M {M.shape}, ICU mean={Y_ICU.mean():.3f}, LOS mean={Y_LOS.mean():.2f}\")\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:12:12.359629Z","iopub.execute_input":"2025-10-28T06:12:12.359949Z","iopub.status.idle":"2025-10-28T06:12:24.948717Z","shell.execute_reply.started":"2025-10-28T06:12:12.359927Z","shell.execute_reply":"2025-10-28T06:12:24.947795Z"}},"outputs":[{"name":"stdout","text":"📂 Reading MIMIC-IV tables from: /kaggle/input/mimic-iv-2-1/mimic-iv-2.1\n✅ Orphanet rare-disease subset: 3781 patients | 5923 admissions\nAdmissions shape: (5923, 8)\n✅ Final tensors built: X (3781, 25, 3), M (3781, 25), ICU mean=0.321, LOS mean=1.95\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"96"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# ============================================================\n# 5) Fixed PyTorch Dataset for TinyMamba (with x_true return)\n# ============================================================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass EHRTinyDataset(Dataset):\n    def __init__(self, X, M, mask_prob=0.15):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.M = torch.tensor(M, dtype=torch.float32)\n        self.mask_prob = mask_prob\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx].clone()\n        mask = self.M[idx].clone()\n\n        # randomly mask some time steps\n        mask_rand = (torch.rand_like(mask) < self.mask_prob).float()\n        x_masked = x.clone()\n        x_masked[mask_rand.bool()] = 0.0  # zero out masked steps\n\n        return x_masked, x, mask  # return both masked & true versions\n\n# Build dataloader\ntrain_ds = EHRTinyDataset(X, M, mask_prob=CFG.MASK_PROB)\ntrain_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, shuffle=True)\n\nprint(f\"✅ Dataset ready: {len(train_ds)} patients | batch size {CFG.BATCH_SIZE}\")\nprint(\"Example batch shapes:\")\nxb, x_true, m = next(iter(train_loader))\nprint(\"x_masked:\", xb.shape, \"x_true:\", x_true.shape, \"mask:\", m.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:14:22.787936Z","iopub.execute_input":"2025-10-28T06:14:22.788565Z","iopub.status.idle":"2025-10-28T06:14:22.803005Z","shell.execute_reply.started":"2025-10-28T06:14:22.788535Z","shell.execute_reply":"2025-10-28T06:14:22.802180Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset ready: 3781 patients | batch size 64\nExample batch shapes:\nx_masked: torch.Size([64, 25, 3]) x_true: torch.Size([64, 25, 3]) mask: torch.Size([64, 25])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ============================================================\n# Cell 6) TinyMamba for continuous features (no external deps)\n# Encoder: proj (D->d_model) + pos + L TinyMamba blocks -> CLS pool\n# Decoder: d_model->D to reconstruct masked steps (MSE loss)\n# ============================================================\nimport math\nimport torch\nimport torch.nn as nn\n\nclass TinyMamba(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.fc1 = nn.Linear(d_model, d_model)\n        self.fc2 = nn.Linear(d_model, d_model)\n        self.gate = nn.Sigmoid()\n        self.norm = nn.LayerNorm(d_model)\n        self.drop = nn.Dropout(0.1)\n\n    def forward(self, x):      # (B, T, d)\n        y = self.gate(self.fc1(x)) * self.fc2(x)\n        return self.norm(x + self.drop(y))\n\nclass EHR_MambaCont(nn.Module):\n    def __init__(self, d_in, d_model=256, n_layers=4, max_T=2048):\n        super().__init__()\n        self.proj = nn.Linear(d_in, d_model)\n        self.pos  = nn.Embedding(max_T, d_model)\n        self.blocks = nn.ModuleList([TinyMamba(d_model) for _ in range(n_layers)])\n        self.norm = nn.LayerNorm(d_model)\n        self.dec  = nn.Linear(d_model, d_in)           # decoder for reconstruction\n\n    def forward(self, x, attn_mask=None):\n        \"\"\"\n        x: (B, T, D)  continuous features; already zero where masked\n        attn_mask: (B, T) 1.0 for observed timesteps, 0.0 for padded (optional)\n        \"\"\"\n        B, T, D = x.size()\n        pos_ids = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n        h = self.proj(x) + self.pos(pos_ids)           # (B,T,d)\n        for blk in self.blocks:\n            h = blk(h)\n        h = self.norm(h)\n        # CLS = timestep 0 representation (you already have chronological sorting)\n        cls = h[:, 0]                                   # (B, d)\n        recon = self.dec(h)                             # (B, T, D)\n        return recon, cls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:14:18.522993Z","iopub.execute_input":"2025-10-28T06:14:18.523305Z","iopub.status.idle":"2025-10-28T06:14:18.533477Z","shell.execute_reply.started":"2025-10-28T06:14:18.523283Z","shell.execute_reply":"2025-10-28T06:14:18.532721Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ============================================================\n# Cell 7) Train masked reconstruction on observed timesteps only (fixed)\n# ============================================================\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\n\n# --- Initialize model ---\nd_model = CFG.D_MODEL\nn_layers = CFG.N_LAYERS\nmodel = EHR_MambaCont(d_in=X.shape[2], d_model=d_model, n_layers=n_layers, max_T=X.shape[1]).to(device)\n\nopt = torch.optim.AdamW(model.parameters(), lr=CFG.LR, weight_decay=1e-2)\nmse = nn.MSELoss(reduction=\"none\")\n\nEPOCHS = CFG.EPOCHS\nmodel.train()\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"[TinyMamba] Epoch {epoch}\")\n    for xb, x_true, mask in pbar:  # <-- uses tuple output from new dataset\n        xb, x_true, mask = xb.to(device), x_true.to(device), mask.to(device)\n\n        opt.zero_grad(set_to_none=True)\n\n        recon, _ = model(xb)  # (B,T,D)\n\n        # compute reconstruction loss only on masked & valid timesteps\n        mask_3d = mask.unsqueeze(-1)  # (B,T,1)\n        loss_mat = mse(recon, x_true) * mask_3d\n        loss = loss_mat.sum() / mask_3d.sum().clamp_min(1.0)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n\n        epoch_loss += loss.item()\n        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n    print(f\"✅ Epoch {epoch} | Mean Loss: {epoch_loss / len(train_loader):.4f}\")\n\nprint(\"🎯 Training complete.\")\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:19:30.315557Z","iopub.execute_input":"2025-10-28T06:19:30.315868Z","iopub.status.idle":"2025-10-28T06:20:01.597905Z","shell.execute_reply.started":"2025-10-28T06:19:30.315846Z","shell.execute_reply":"2025-10-28T06:20:01.597092Z"}},"outputs":[{"name":"stderr","text":"[TinyMamba] Epoch 1: 100%|██████████| 59/59 [00:00<00:00, 91.85it/s, loss=0.2153]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 1 | Mean Loss: 0.6683\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 2: 100%|██████████| 59/59 [00:00<00:00, 93.84it/s, loss=0.2247]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 2 | Mean Loss: 0.2628\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 3: 100%|██████████| 59/59 [00:00<00:00, 93.92it/s, loss=0.2643]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 3 | Mean Loss: 0.2315\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 4: 100%|██████████| 59/59 [00:00<00:00, 95.77it/s, loss=0.2214]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 4 | Mean Loss: 0.2149\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 5: 100%|██████████| 59/59 [00:00<00:00, 92.48it/s, loss=0.3342]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 5 | Mean Loss: 0.1947\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 6: 100%|██████████| 59/59 [00:00<00:00, 95.34it/s, loss=0.3713]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 6 | Mean Loss: 0.1996\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 7: 100%|██████████| 59/59 [00:00<00:00, 94.26it/s, loss=0.2035]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 7 | Mean Loss: 0.1878\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 8: 100%|██████████| 59/59 [00:00<00:00, 94.26it/s, loss=0.1173]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 8 | Mean Loss: 0.1766\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 9: 100%|██████████| 59/59 [00:00<00:00, 91.33it/s, loss=0.1329]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 9 | Mean Loss: 0.1781\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 10: 100%|██████████| 59/59 [00:00<00:00, 94.13it/s, loss=0.1449]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 10 | Mean Loss: 0.1647\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 11: 100%|██████████| 59/59 [00:00<00:00, 95.06it/s, loss=0.1046]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 11 | Mean Loss: 0.1750\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 12: 100%|██████████| 59/59 [00:00<00:00, 96.36it/s, loss=0.1059]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 12 | Mean Loss: 0.1568\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 13: 100%|██████████| 59/59 [00:00<00:00, 95.70it/s, loss=0.1491]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 13 | Mean Loss: 0.1658\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 14: 100%|██████████| 59/59 [00:00<00:00, 94.24it/s, loss=0.1944]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 14 | Mean Loss: 0.1646\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 15: 100%|██████████| 59/59 [00:00<00:00, 96.36it/s, loss=0.0992]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 15 | Mean Loss: 0.1711\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 16: 100%|██████████| 59/59 [00:00<00:00, 94.04it/s, loss=0.2120]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 16 | Mean Loss: 0.1803\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 17: 100%|██████████| 59/59 [00:00<00:00, 98.42it/s, loss=0.1428] \n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 17 | Mean Loss: 0.1668\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 18: 100%|██████████| 59/59 [00:00<00:00, 94.41it/s, loss=0.0713]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 18 | Mean Loss: 0.1613\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 19: 100%|██████████| 59/59 [00:00<00:00, 94.75it/s, loss=0.1223]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 19 | Mean Loss: 0.1640\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 20: 100%|██████████| 59/59 [00:00<00:00, 97.53it/s, loss=0.1360]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 20 | Mean Loss: 0.1528\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 21: 100%|██████████| 59/59 [00:00<00:00, 91.61it/s, loss=0.1179]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 21 | Mean Loss: 0.1619\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 22: 100%|██████████| 59/59 [00:00<00:00, 96.14it/s, loss=0.2105]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 22 | Mean Loss: 0.1560\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 23: 100%|██████████| 59/59 [00:00<00:00, 97.73it/s, loss=0.0711]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 23 | Mean Loss: 0.1681\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 24: 100%|██████████| 59/59 [00:00<00:00, 91.37it/s, loss=0.0866]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 24 | Mean Loss: 0.1578\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 25: 100%|██████████| 59/59 [00:00<00:00, 90.62it/s, loss=0.1783]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 25 | Mean Loss: 0.1598\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 26: 100%|██████████| 59/59 [00:00<00:00, 97.23it/s, loss=0.1986]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 26 | Mean Loss: 0.1568\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 27: 100%|██████████| 59/59 [00:00<00:00, 96.83it/s, loss=0.1225]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 27 | Mean Loss: 0.1493\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 28: 100%|██████████| 59/59 [00:00<00:00, 95.55it/s, loss=0.1192]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 28 | Mean Loss: 0.1529\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 29: 100%|██████████| 59/59 [00:00<00:00, 95.76it/s, loss=0.0729]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 29 | Mean Loss: 0.1613\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 30: 100%|██████████| 59/59 [00:00<00:00, 95.48it/s, loss=0.0827]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 30 | Mean Loss: 0.1419\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 31: 100%|██████████| 59/59 [00:00<00:00, 97.46it/s, loss=0.1661]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 31 | Mean Loss: 0.1571\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 32: 100%|██████████| 59/59 [00:00<00:00, 93.10it/s, loss=0.1303]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 32 | Mean Loss: 0.1567\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 33: 100%|██████████| 59/59 [00:00<00:00, 93.61it/s, loss=0.1582]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 33 | Mean Loss: 0.1493\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 34: 100%|██████████| 59/59 [00:00<00:00, 93.95it/s, loss=0.1978]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 34 | Mean Loss: 0.1507\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 35: 100%|██████████| 59/59 [00:00<00:00, 93.26it/s, loss=0.1256]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 35 | Mean Loss: 0.1572\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 36: 100%|██████████| 59/59 [00:00<00:00, 92.84it/s, loss=0.1786]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 36 | Mean Loss: 0.1485\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 37: 100%|██████████| 59/59 [00:00<00:00, 95.37it/s, loss=0.1602]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 37 | Mean Loss: 0.1464\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 38: 100%|██████████| 59/59 [00:00<00:00, 96.35it/s, loss=0.1363]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 38 | Mean Loss: 0.1453\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 39: 100%|██████████| 59/59 [00:00<00:00, 94.38it/s, loss=0.1454]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 39 | Mean Loss: 0.1573\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 40: 100%|██████████| 59/59 [00:00<00:00, 93.25it/s, loss=0.0503]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 40 | Mean Loss: 0.1384\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 41: 100%|██████████| 59/59 [00:00<00:00, 91.55it/s, loss=0.2225]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 41 | Mean Loss: 0.1606\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 42: 100%|██████████| 59/59 [00:00<00:00, 94.69it/s, loss=0.1307]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 42 | Mean Loss: 0.1425\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 43: 100%|██████████| 59/59 [00:00<00:00, 94.30it/s, loss=0.0735]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 43 | Mean Loss: 0.1559\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 44: 100%|██████████| 59/59 [00:00<00:00, 97.94it/s, loss=0.1767] \n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 44 | Mean Loss: 0.1497\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 45: 100%|██████████| 59/59 [00:00<00:00, 96.40it/s, loss=0.1864]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 45 | Mean Loss: 0.1404\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 46: 100%|██████████| 59/59 [00:00<00:00, 95.01it/s, loss=0.1397]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 46 | Mean Loss: 0.1498\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 47: 100%|██████████| 59/59 [00:00<00:00, 95.34it/s, loss=0.1744]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 47 | Mean Loss: 0.1578\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 48: 100%|██████████| 59/59 [00:00<00:00, 97.04it/s, loss=0.0799]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 48 | Mean Loss: 0.1435\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 49: 100%|██████████| 59/59 [00:00<00:00, 96.69it/s, loss=0.1713]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 49 | Mean Loss: 0.1618\n","output_type":"stream"},{"name":"stderr","text":"[TinyMamba] Epoch 50: 100%|██████████| 59/59 [00:00<00:00, 97.13it/s, loss=0.1887]","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 50 | Mean Loss: 0.1562\n🎯 Training complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# ============================================================\n# 5) Correct TinyMamba Dataset (final)\n# ============================================================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass EHRTinyDataset(Dataset):\n    def __init__(self, X, M, mask_prob=0.15):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.M = torch.tensor(M, dtype=torch.float32)\n        self.mask_prob = mask_prob\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx].clone()\n        mask = self.M[idx].clone()\n\n        # Randomly drop some time steps\n        mask_rand = (torch.rand_like(mask) < self.mask_prob).float()\n        x_masked = x.clone()\n        # expand mask_rand to match feature dimension\n        x_masked[mask_rand.bool(), :] = 0.0  \n\n        return x_masked, x, mask  # masked input, true input, mask\n\n# --- dataloader\ntrain_ds = EHRTinyDataset(X, M, mask_prob=CFG.MASK_PROB)\ntrain_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, shuffle=True, drop_last=True)\n\nprint(f\"✅ Dataset ready: {len(train_ds)} patients | batch size {CFG.BATCH_SIZE}\")\nxb, x_true, m = next(iter(train_loader))\nprint(\"x_masked:\", xb.shape, \"| x_true:\", x_true.shape, \"| mask:\", m.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:15:03.669167Z","iopub.execute_input":"2025-10-28T06:15:03.669844Z","iopub.status.idle":"2025-10-28T06:15:03.685109Z","shell.execute_reply.started":"2025-10-28T06:15:03.669815Z","shell.execute_reply":"2025-10-28T06:15:03.683837Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset ready: 3781 patients | batch size 64\nx_masked: torch.Size([64, 25, 3]) | x_true: torch.Size([64, 25, 3]) | mask: torch.Size([64, 25])\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# --- remove any old version of EHRTinyDataset still in memory ---\ndel EHRTinyDataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:15:40.873586Z","iopub.execute_input":"2025-10-28T06:15:40.874295Z","iopub.status.idle":"2025-10-28T06:15:40.878311Z","shell.execute_reply.started":"2025-10-28T06:15:40.874270Z","shell.execute_reply":"2025-10-28T06:15:40.877372Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# ============================================================\n# 5) Correct TinyMamba Dataset (final and persistent)\n# ============================================================\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass EHRTinyDataset(Dataset):\n    def __init__(self, X, M, mask_prob=0.15):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.M = torch.tensor(M, dtype=torch.float32)\n        self.mask_prob = mask_prob\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx].clone()\n        mask = self.M[idx].clone()\n        mask_rand = (torch.rand_like(mask) < self.mask_prob).float()\n\n        # expand mask_rand to same shape as x for feature masking\n        mask_rand_expanded = mask_rand.unsqueeze(-1).expand_as(x)\n        x_masked = x.clone()\n        x_masked[mask_rand_expanded.bool()] = 0.0\n\n        return x_masked, x, mask\n\n# --- Build DataLoader ---\ntrain_ds = EHRTinyDataset(X, M, mask_prob=CFG.MASK_PROB)\ntrain_loader = DataLoader(train_ds, batch_size=CFG.BATCH_SIZE, shuffle=True, drop_last=True)\n\nprint(f\"✅ Dataset ready: {len(train_ds)} patients | batch size {CFG.BATCH_SIZE}\")\nxb, x_true, m = next(iter(train_loader))\nprint(\"x_masked:\", xb.shape, \"| x_true:\", x_true.shape, \"| mask:\", m.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:15:48.552021Z","iopub.execute_input":"2025-10-28T06:15:48.552691Z","iopub.status.idle":"2025-10-28T06:15:48.568102Z","shell.execute_reply.started":"2025-10-28T06:15:48.552665Z","shell.execute_reply":"2025-10-28T06:15:48.567219Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset ready: 3781 patients | batch size 64\nx_masked: torch.Size([64, 25, 3]) | x_true: torch.Size([64, 25, 3]) | mask: torch.Size([64, 25])\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}